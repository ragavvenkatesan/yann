    #################
    # Boiler PLate  #
    #################
    
## Boiler Plate ## 
if __name__ == '__main__':
    
    import sys

                                                                                            # for epoch in [0, mom_epoch_interval] the momentum increases linearly
    optimization_params = {
                            "mom_start"                         : 0.5,                      # from mom_start to mom_end. After mom_epoch_interval, it stay at mom_end
                            "mom_end"                           : 0.98,
                            "mom_interval"                      : 50,
                            "mom_type"                          : 1,                        # if mom_type = 1 , classical momentum if mom_type = 0, no momentum, if mom_type = 2 Nesterov's accelerated gradient momentum ( not yet done... something is wrong.. refer code)
                            "initial_learning_rate"             : 0.01,                      # Learning rate at the start
                            "learning_rate_at_ada_grad_begin"   : 1,                        # Learning rate once ada_grad starts. Although ada grad can start anywhere, I recommend at the begining.
                            "learning_rate_at_ada_grad_end"     : 0.01,                      # learning rate once ada grad ends and regular SGD begins again. 
                            "learning_rate_decay"               : 0.998, 
                            "regularize_l1"                     : False,                     # only for the last logistic layer
                            "regularize_l2"                     : False,                     # only for the last logistic layer 
                            "cross_entropy"                     : False,                     # use for binary labels only. 
                            "ada_grad_begin"                    : 1,                        # Which epoch to begin ada grad. Don't use adagrad when using dropout. 
                            "ada_grad_end"                      : 0,                       # Which epoch to end ada grad
                            "fudge_factor"                      : 1e-7,                     # Just to avoid divide by zero, but google even advocates trying '1'                            
                            }



    filename_params ={ 
                        "results_file_name" : "../results/results_cifar10.txt",        # Files that will be saved down on completion Can be used by the parse.m file
                        "error_file_name"   : "../results/error_cifar10.txt",
                        "cost_file_name"    : "../results/cost_cifar10.txt"
                    }        
        
    data_params = {
                   "type"               : 'skdata',                                    # Options: 'pkl', 'skdata' , 'mat' for loading pkl files, mat files for skdata files.
                   "loc"                : 'cifar10',                             # location for mat or pkl files, which data for skdata files. Skdata will be downloaded and used from '~/.skdata/'
                   "batch_size"         : 500,                                      # For loading and for Gradient Descent Batch Size
                   "load_batches"       : -1, 
                   "batches2train"      : 80,                                      # Number of training batches.
                   "batches2test"       : 20,                                       # Number of testing batches.
                   "batches2validate"   : 20,                                       # Number of validation batches
                   "height"             : 32,                                       # Height of each input image
                   "width"              : 32,                                       # Width of each input image
                   "channels"           : 3                                         # Number of channels of each input image 
                  }

    arch_params = {
                       # Decay of Learninig rate after each epoch of SGD
                    "squared_filter_length_limit"       : 15,   
                    "n_epochs"                          : 200,                      # Total Number of epochs to run before completion (no premature completion)
                    "validate_after_epochs"             : 1,                        # After how many iterations to calculate validation set accuracy ?
                    "mlp_activations"                   : [ ReLU, ReLU , ReLU ],           # Activations of MLP layers Options: ReLU, Sigmoid, Tanh
                    "cnn_activations"                   : [ ReLU, ReLU ],           # Activations for CNN layers Options: ReLU, Sigmoid, Tanh
                    "dropout"                           : True,                    # Flag for dropout / backprop When using dropout, don't use adagrad.
                    "column_norm"                       : False,
                    "dropout_rates"                     : [ 0.5, 0.5 ,0.5 ],             # Rates of dropout. Use 0 is backprop.
                    "nkerns"                            : [ 20 , 50 ],              # Number of feature maps at each CNN layer
                    "outs"                              : 10,                       # Number of output nodes ( must equal number of classes)
                    "filter_size"                       : [  5, 5 ],                # Receptive field of each CNN layer
                    "pooling_size"                      : [  2, 2 ],                # Pooling field of each CNN layer
                    "num_nodes"                         : [  1000, 1000  ],                # Number of nodes in each MLP layer
                    "use_bias"                          : True,                     # Flag for using bias                   
                    "random_seed"                       : 23455,                    # Use same seed for reproduction of results.
                    "svm_flag"                          : False                     # True makes the last layer a SVM

                 }

    run_cnn(
                    arch_params         = arch_params,
                    optimization_params = optimization_params,
                    data_params         = data_params, 
                    filename_params     = filename_params,
                    verbose             = False                                                  # True prints in a lot of intermetediate steps, False keeps it to minimum.
                )


